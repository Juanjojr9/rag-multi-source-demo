
# ğŸ±ğŸ” RAG Multi-Source â€“ Demo

## DescripciÃ³n

RAG Multiâ€‘Source Demo es un pequeÃ±o proyecto educativo que muestra cÃ³mo combinar Retrievalâ€‘Augmented Generation con un stack Python moderno. Permite indexar documentos locales y pÃ¡ginas web, recuperar los fragmentos mÃ¡s relevantes con FAISS y generar respuestas contextuales usando un LLM ligero (TinyLlamaâ€‘1.1Bâ€‘Chat).

Ideal para:

Probar arquitecturas RAG sin depender de servicios externos.
Aprender a integrar FAISS, LangChain y FastAPI.
Desplegar una demo completa (API + UI) en minutos.



| Componente | DescripciÃ³n | TecnologÃ­as |
|------------|-------------|-------------|
| **`build_index.py`** | Lee todos los ficheros de `data/`, descarga opcionalmente URLs, fragmenta los textos y genera un Ã­ndice **FAISS** | LangChain 0.2 Â· sentence-transformers |
| **`rag_server.py`** | API REST  â€“ `GET /` (estado) Â· `GET /ask` (respuesta + fuentes) | FastAPI Â· Uvicorn |
| **`client.py`** | Interfaz Streamlit para chatear con el RAG | Streamlit 1.35 |
| **`models.py`** | Carga el LLM (`TinyLlama-1.1B-Chat`) y prepara el prompt | HuggingFace Transformers |
| **`vector_store/`** | Ãndice FAISS persistente generado por el script | FAISS-CPU |

---
##âš¡ Prerrequisitos

PythonÂ â‰¥Â 3.11

OllamaÂ â‰¥Â 0.1.30 para descargar y ejecutar el LLM localmente

Instala Ollama
Windows: descarga OllamaSetup.exe desde la pÃ¡gina oficial e instala.

(macOS/Linux/WSL): curl -fsSL https://ollama.com/install.sh | sh

(Opcional) GPU con CUDAÂ 11+ para acelerar TinyLlama

2â€‘3Â GB de RAM libres para el Ã­ndice y el modelo cuantizado


---
## ğŸš€ InstalaciÃ³n rÃ¡pida

```bash
git clone https://github.com/Juanjojr9/rag-multi-source-demo.git
cd  rag-multi-source-demo
python -m venv .venv
```
 Windows:
.\.venv\Scripts\activate
macOS / Linux:
source .venv/bin/activate
```bash
pip install -r requirements.txt
```
DespuÃ©s abre PowerShell o CMD y ejecuta
```bash
ollama pull tinyllama:chat     # LLM
ollama pull all-minilm         # Embeddings
```

### ğŸ”§ Construir el Ã­ndice (solo la primera vez)


Copia tus PDFs, TXT, MDâ€¦ en ./data o usa una URL

```bash
python build_index.py --data_dir ./data \
                      --urls "https://es.wikipedia.org/wiki/Gato"
```
El script crea data/ si no existe y genera vector_store/ con los vectores.


EjecuciÃ³n
### 1) Servidor RAG
```bash
uvicorn rag_server:app --reload         # Swagger: http://localhost:8000/docs
```
### 2) Cliente Streamlit (en otra terminal)
```bash
streamlit run client.py                 # UI:     http://localhost:8501
```

## âœ¨ Opciones de ampliaciÃ³n
Cambiar el modelo de embeddings (EMB_MODEL).

Sustituir TinyLlama por otro LLM en models.py.

AÃ±adir mÃ¡s ficheros o URLs y volver a ejecutar build_index.py.

Contenerizar con Docker para despliegue sencillo (no incluido).


## ğŸ—‚ï¸ Estructura
```bash
â”œâ”€â”€ build_index.py      # IndexaciÃ³n + ingesta web
â”œâ”€â”€ rag_server.py       # API FastAPI
â”œâ”€â”€ client.py           # Front Streamlit
â”œâ”€â”€ models.py           # Carga LLM + generaciÃ³n
â”œâ”€â”€ requirements.txt
â””â”€â”€ data/ | vector_store/
```

## âš™ï¸ Ajustes rÃ¡pidos
Variable	DÃ³nde	DescripciÃ³n
EMB_MODEL	build_index.py, rag_server.py	Cambia modelo de embeddings
INDEX_PATH	rag_server.py	Ruta al Ã­ndice FAISS
API_URL	client.py	End-point del servidor en producciÃ³n


## ğŸ¤ Contribuir
Â¡Se aceptan PRs! Abre una issue primero para discutir cambios mayores.
AsegÃºrate de ejecutar pre-commit y aÃ±adir tests cuando corresponda.

